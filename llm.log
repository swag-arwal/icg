2025-11-07 22:56:34,497 - __main__ - INFO - ============================================================
2025-11-07 22:56:34,498 - __main__ - INFO - Starting LLM RAG Pipeline
2025-11-07 22:56:34,498 - __main__ - INFO - ============================================================
2025-11-07 22:56:34,498 - __main__ - INFO - Processing 6 files
2025-11-07 22:56:34,498 - __main__ - INFO - Step 1: Loading tables from 6 files...
2025-11-07 22:56:34,936 - __main__ - INFO - Loaded 6 tables
2025-11-07 22:56:34,936 - __main__ - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-07 22:56:34,961 - __main__ - INFO - Created 150 chunks
2025-11-07 22:56:34,963 - __main__ - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-07 22:56:34,963 - __main__ - INFO - Files have changed or cache not found, will regenerate embeddings
2025-11-07 22:56:34,963 - __main__ - INFO - Generating embeddings using model: all-MiniLM-L6-v2
2025-11-07 22:56:34,965 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-07 22:56:34,966 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-07 22:56:41,736 - __main__ - INFO - Encoding 150 chunks...
2025-11-07 22:56:55,710 - __main__ - INFO - Saving embeddings and index to cache...
2025-11-07 22:56:55,712 - __main__ - INFO - FAISS index saved to cache\faiss_index.bin
2025-11-07 22:56:55,714 - __main__ - INFO - Embeddings saved to cache\embeddings.npy
2025-11-07 22:56:55,720 - __main__ - INFO - Chunks saved to cache\chunks.pkl
2025-11-07 22:56:55,721 - __main__ - INFO - Model name saved: all-MiniLM-L6-v2
2025-11-07 22:56:55,749 - __main__ - INFO - Cache metadata saved successfully
2025-11-07 22:56:55,756 - __main__ - INFO - Step 4: Retrieving top 3 results for: 'what percentage of children are enrolled in government primary schools in india in 2024'
2025-11-07 22:56:55,855 - __main__ - INFO - Retrieved 3 chunks:
2025-11-07 22:56:55,862 - __main__ - INFO - Step 5: Generating final LLM prompt...
2025-11-07 22:56:55,864 - __main__ - INFO - Calling LLM for answer...
2025-11-07 22:56:55,866 - __main__ - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 22:56:57,340 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 22:56:59,369 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 22:56:59,430 - __main__ - INFO - 
============================================================
2025-11-07 22:56:59,459 - __main__ - INFO - FINAL RESULTS
2025-11-07 22:56:59,465 - __main__ - INFO - ============================================================
2025-11-07 22:56:59,480 - __main__ - INFO - Pipeline completed successfully
2025-11-07 22:58:45,055 - __main__ - INFO - ============================================================
2025-11-07 22:58:45,056 - __main__ - INFO - Starting LLM RAG Pipeline
2025-11-07 22:58:45,056 - __main__ - INFO - ============================================================
2025-11-07 22:58:45,056 - __main__ - INFO - Processing 6 files
2025-11-07 22:58:45,056 - __main__ - INFO - Step 1: Loading tables from 6 files...
2025-11-07 22:58:45,101 - __main__ - INFO - Loaded 6 tables
2025-11-07 22:58:45,101 - __main__ - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-07 22:58:45,205 - __main__ - INFO - Created 150 chunks
2025-11-07 22:58:45,206 - __main__ - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-07 22:58:45,287 - __main__ - INFO - All files unchanged, can use cache
2025-11-07 22:58:45,287 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-07 22:58:45,287 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-07 22:58:45,319 - __main__ - INFO - FAISS index loaded: 150 vectors
2025-11-07 22:58:45,353 - __main__ - INFO - Embeddings loaded: shape (150, 384)
2025-11-07 22:58:45,547 - __main__ - INFO - Chunks loaded: 150 chunks
2025-11-07 22:58:45,574 - __main__ - INFO - Model name: all-MiniLM-L6-v2
2025-11-07 22:58:45,596 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-07 22:58:45,600 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-07 22:58:51,065 - __main__ - INFO - Step 4: Retrieving top 3 results for: 'what percentage of children are enrolled in government primary schools in india in 2024'
2025-11-07 22:58:51,343 - __main__ - INFO - Retrieved 3 chunks:
2025-11-07 22:58:51,629 - __main__ - INFO - Step 5: Generating final LLM prompt...
2025-11-07 22:58:51,645 - __main__ - INFO - Calling LLM for answer...
2025-11-07 22:58:51,658 - __main__ - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 22:58:53,320 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 22:58:56,816 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 22:58:56,837 - __main__ - INFO - 
============================================================
2025-11-07 22:58:56,838 - __main__ - INFO - FINAL RESULTS
2025-11-07 22:58:56,838 - __main__ - INFO - ============================================================
2025-11-07 22:58:56,844 - __main__ - INFO - Pipeline completed successfully
2025-11-07 23:08:18,242 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-07 23:08:18,580 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-07 23:08:18,659 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-07 23:08:18,695 - llm - INFO - All files unchanged, can use cache
2025-11-07 23:08:18,695 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:08:18,698 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:08:18,707 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-07 23:08:18,709 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-07 23:08:18,713 - llm - INFO - Chunks loaded: 150 chunks
2025-11-07 23:08:18,716 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-07 23:08:18,755 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-07 23:08:18,759 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-07 23:08:59,680 - llm - INFO - Step 4: Retrieving top 3 results for: 'so what canyou'
2025-11-07 23:09:00,112 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:09:00,288 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:09:00,291 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:09:00,858 - llm - INFO - Step 4: Retrieving top 3 results for: 'so what canyou'
2025-11-07 23:09:02,945 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:09:03,818 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:09:03,831 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:09:07,835 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:09:08,348 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:09:09,350 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:09:09,909 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:09:44,412 - llm - INFO - Step 4: Retrieving top 3 results for: 'what I meant was, how's that stats bitch?'
2025-11-07 23:09:44,547 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:09:44,604 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:09:44,604 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:09:47,601 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:09:49,501 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:22:22,836 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-07 23:22:23,109 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-07 23:22:23,163 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-07 23:22:23,176 - llm - INFO - All files unchanged, can use cache
2025-11-07 23:22:23,177 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:22:23,177 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:22:23,183 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-07 23:22:23,184 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-07 23:22:23,187 - llm - INFO - Chunks loaded: 150 chunks
2025-11-07 23:22:23,189 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-07 23:22:23,207 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-07 23:22:23,208 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-07 23:23:05,442 - llm - INFO - Step 4: Retrieving top 3 results for: 'hi'
2025-11-07 23:23:06,974 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:23:07,063 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:23:07,063 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:23:10,670 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:23:12,405 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:23:58,554 - llm - INFO - Step 4: Retrieving top 3 results for: 'heyhe'
2025-11-07 23:23:58,665 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:23:58,722 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:23:58,725 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:24:02,837 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:24:04,533 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:25:05,367 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-07 23:25:05,502 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-07 23:25:05,543 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-07 23:25:05,556 - llm - INFO - All files unchanged, can use cache
2025-11-07 23:25:05,556 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:25:05,557 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:25:05,559 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-07 23:25:05,559 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-07 23:25:05,562 - llm - INFO - Chunks loaded: 150 chunks
2025-11-07 23:25:05,563 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-07 23:25:05,576 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-07 23:25:05,576 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-07 23:31:58,416 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-07 23:31:58,593 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-07 23:31:58,671 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-07 23:31:58,689 - llm - INFO - All files unchanged, can use cache
2025-11-07 23:31:58,689 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:31:58,691 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:31:58,692 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-07 23:31:58,694 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-07 23:31:58,702 - llm - INFO - Chunks loaded: 150 chunks
2025-11-07 23:31:58,704 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-07 23:31:58,745 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-07 23:31:58,746 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-07 23:32:14,815 - llm - INFO - Step 4: Retrieving top 3 results for: 'hi'
2025-11-07 23:32:15,043 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:32:15,065 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:32:15,066 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:32:17,750 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:32:19,587 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:32:40,604 - llm - INFO - Step 4: Retrieving top 3 results for: 'hi'
2025-11-07 23:32:40,844 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:32:40,864 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:32:40,865 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:32:43,614 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:32:44,744 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 429 Too Many Requests"
2025-11-07 23:32:44,748 - llm - ERROR - Error getting LLM answer: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}
2025-11-07 23:33:16,392 - llm - INFO - Step 4: Retrieving top 3 results for: 'hi'
2025-11-07 23:33:16,589 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:33:16,626 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:33:16,628 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:33:19,479 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:33:22,402 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:34:56,541 - llm - INFO - Step 4: Retrieving top 3 results for: 'what's up with punjab?'
2025-11-07 23:34:56,685 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:34:56,725 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:34:56,725 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:34:59,894 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:35:09,727 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:47:29,066 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-07 23:47:29,186 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-07 23:47:29,279 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-07 23:47:29,302 - llm - INFO - All files unchanged, can use cache
2025-11-07 23:47:29,302 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:47:29,304 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:47:29,313 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-07 23:47:29,317 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-07 23:47:29,322 - llm - INFO - Chunks loaded: 150 chunks
2025-11-07 23:47:29,327 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-07 23:47:29,374 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-07 23:47:29,378 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-07 23:48:01,152 - llm - INFO - Step 4: Retrieving top 3 results for: 'hey'
2025-11-07 23:48:02,887 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:48:03,070 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:48:03,071 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:48:05,985 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:48:07,934 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-07 23:53:40,760 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-07 23:53:41,242 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-07 23:53:41,372 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-07 23:53:41,422 - llm - INFO - All files unchanged, can use cache
2025-11-07 23:53:41,423 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:53:41,424 - llm - INFO - Loading embeddings and index from cache...
2025-11-07 23:53:41,426 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-07 23:53:41,431 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-07 23:53:41,439 - llm - INFO - Chunks loaded: 150 chunks
2025-11-07 23:53:41,440 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-07 23:53:41,523 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-07 23:53:41,526 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-07 23:54:27,380 - llm - INFO - Step 4: Retrieving top 3 results for: 'hi'
2025-11-07 23:54:27,522 - llm - INFO - Retrieved 3 chunks:
2025-11-07 23:54:27,548 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-07 23:54:27,550 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-07 23:54:30,360 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-07 23:54:32,379 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-08 00:01:27,710 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 00:01:27,876 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 00:01:27,966 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 00:01:27,993 - llm - INFO - All files unchanged, can use cache
2025-11-08 00:01:28,043 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 00:01:28,045 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 00:01:28,058 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 00:01:28,089 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 00:01:28,107 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 00:01:28,120 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 00:01:28,174 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 00:01:28,176 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 00:05:04,563 - llm - INFO - Step 4: Retrieving top 3 results for: 'hi'
2025-11-08 00:05:04,661 - llm - INFO - Retrieved 3 chunks:
2025-11-08 00:05:04,671 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 00:05:04,672 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-08 00:05:07,027 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-08 00:05:09,115 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-08 00:06:49,682 - llm - INFO - Step 4: Retrieving top 3 results for: 'hey'
2025-11-08 00:06:49,868 - llm - INFO - Retrieved 3 chunks:
2025-11-08 00:06:49,883 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 00:06:49,883 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-08 00:06:52,491 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-08 00:06:54,202 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-08 00:08:08,670 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 00:08:09,189 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 00:08:09,265 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 00:08:09,282 - llm - INFO - All files unchanged, can use cache
2025-11-08 00:08:09,283 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 00:08:09,287 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 00:08:09,288 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 00:08:09,292 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 00:08:09,298 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 00:08:09,303 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 00:08:09,323 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 00:08:09,324 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 00:52:37,109 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 00:52:37,201 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 00:52:37,242 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 00:52:37,279 - llm - INFO - All files unchanged, can use cache
2025-11-08 00:52:37,280 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 00:52:37,280 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 00:52:37,321 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 00:52:37,343 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 00:52:37,392 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 00:52:37,396 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 00:52:37,412 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 00:52:37,413 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 00:53:01,667 - llm - INFO - Step 4: Retrieving top 3 results for: 'hey bitch?'
2025-11-08 00:53:02,009 - llm - INFO - Retrieved 3 chunks:
2025-11-08 00:53:02,027 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 00:53:02,028 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-08 00:53:03,775 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-08 00:53:05,291 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-08 01:09:25,305 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 01:09:25,502 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 01:09:25,587 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 01:09:25,601 - llm - INFO - All files unchanged, can use cache
2025-11-08 01:09:25,601 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 01:09:25,601 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 01:09:25,608 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 01:09:25,613 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 01:09:25,620 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 01:09:25,622 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 01:09:25,691 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 01:09:25,692 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 01:20:43,468 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 01:20:43,821 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 01:20:43,869 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 01:20:43,883 - llm - INFO - All files unchanged, can use cache
2025-11-08 01:20:43,883 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 01:20:43,883 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 01:20:43,894 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 01:20:43,898 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 01:20:43,901 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 01:20:43,902 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 01:20:43,918 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 01:20:43,918 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 01:26:56,865 - __main__ - INFO - ============================================================
2025-11-08 01:26:56,867 - __main__ - INFO - Starting LLM RAG Pipeline
2025-11-08 01:26:56,868 - __main__ - INFO - ============================================================
2025-11-08 01:26:56,868 - __main__ - INFO - Step 1: Loading tables from 6 files...
2025-11-08 01:26:56,936 - __main__ - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 01:26:56,979 - __main__ - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 01:26:56,995 - __main__ - INFO - All files unchanged, can use cache
2025-11-08 01:26:56,995 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-08 01:26:56,997 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-08 01:26:57,006 - __main__ - INFO - FAISS index loaded: 150 vectors
2025-11-08 01:26:57,015 - __main__ - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 01:26:57,020 - __main__ - INFO - Chunks loaded: 150 chunks
2025-11-08 01:26:57,021 - __main__ - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 01:26:57,037 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 01:26:57,037 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 01:27:02,531 - __main__ - INFO - Step 4: Retrieving top 5 results for: 'What are the major schemes in Andhra Pradesh?'
2025-11-08 01:27:02,832 - __main__ - INFO - Retrieved 5 chunks:
2025-11-08 01:27:02,847 - __main__ - INFO - Step 5: Generating final LLM prompt...
2025-11-08 01:27:02,847 - __main__ - INFO - Calling LLM for answer...
2025-11-08 01:27:02,848 - __main__ - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-08 01:27:04,842 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-08 01:27:07,346 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-08 01:27:07,364 - __main__ - INFO - 
============================================================
2025-11-08 01:27:07,366 - __main__ - INFO - FINAL RESULTS
2025-11-08 01:27:07,366 - __main__ - INFO - ============================================================
2025-11-08 01:27:07,367 - __main__ - INFO - Pipeline completed successfully
2025-11-08 01:27:45,049 - __main__ - INFO - ============================================================
2025-11-08 01:27:45,049 - __main__ - INFO - Starting LLM RAG Pipeline
2025-11-08 01:27:45,049 - __main__ - INFO - ============================================================
2025-11-08 01:27:45,049 - __main__ - INFO - Step 1: Loading tables from 6 files...
2025-11-08 01:27:45,095 - __main__ - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 01:27:45,119 - __main__ - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 01:27:45,134 - __main__ - INFO - All files unchanged, can use cache
2025-11-08 01:27:45,134 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-08 01:27:45,134 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-08 01:27:45,136 - __main__ - INFO - FAISS index loaded: 150 vectors
2025-11-08 01:27:45,140 - __main__ - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 01:27:45,142 - __main__ - INFO - Chunks loaded: 150 chunks
2025-11-08 01:27:45,144 - __main__ - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 01:27:45,151 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 01:27:45,151 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 01:27:50,098 - __main__ - INFO - Step 4: Retrieving top 5 results for: 'What are the major schemes in Andhra Pradesh?'
2025-11-08 01:27:50,149 - __main__ - INFO - Retrieved 5 chunks:
2025-11-08 01:27:50,167 - __main__ - INFO - Step 5: Generating final LLM prompt...
2025-11-08 01:27:50,167 - __main__ - INFO - Calling LLM for answer...
2025-11-08 01:27:50,168 - __main__ - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-08 01:27:51,431 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-08 01:27:53,616 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-08 01:27:53,616 - __main__ - INFO - 
============================================================
2025-11-08 01:27:53,616 - __main__ - INFO - FINAL RESULTS
2025-11-08 01:27:53,616 - __main__ - INFO - ============================================================
2025-11-08 01:27:53,616 - __main__ - INFO - Pipeline completed successfully
2025-11-08 01:28:50,905 - __main__ - INFO - ============================================================
2025-11-08 01:28:50,905 - __main__ - INFO - Starting LLM RAG Pipeline
2025-11-08 01:28:50,906 - __main__ - INFO - ============================================================
2025-11-08 01:28:50,906 - __main__ - INFO - Step 1: Loading tables from 6 files...
2025-11-08 01:28:50,908 - __main__ - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 01:28:50,912 - __main__ - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 01:28:50,914 - __main__ - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 01:28:50,917 - __main__ - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 01:28:50,918 - __main__ - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 01:28:50,921 - __main__ - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 01:28:50,921 - __main__ - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 01:28:50,939 - __main__ - INFO - Created 150 total chunks from 6 files.
2025-11-08 01:28:50,940 - __main__ - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 01:28:50,951 - __main__ - INFO - All files unchanged, can use cache
2025-11-08 01:28:50,952 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-08 01:28:50,952 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-08 01:28:50,954 - __main__ - INFO - FAISS index loaded: 150 vectors
2025-11-08 01:28:50,957 - __main__ - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 01:28:50,960 - __main__ - INFO - Chunks loaded: 150 chunks
2025-11-08 01:28:50,963 - __main__ - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 01:28:50,971 - __main__ - INFO - Using cached embeddings and index
2025-11-08 01:28:50,972 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 01:28:50,974 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 01:28:55,248 - __main__ - INFO - Step 4: Retrieving top 5 results for: 'What are the major schemes in Andhra Pradesh?'
2025-11-08 01:28:55,280 - __main__ - INFO - Retrieved 5 chunks:
2025-11-08 01:28:55,281 - __main__ - INFO -   [1] From andhra_pradesh.json | Row 19 | Distance: 0.9270
2025-11-08 01:28:55,281 - __main__ - INFO -   [2] From MP.json | Row 19 | Distance: 1.0272
2025-11-08 01:28:55,281 - __main__ - INFO -   [3] From andhra_pradesh.json | Row 14 | Distance: 1.0300
2025-11-08 01:28:55,281 - __main__ - INFO -   [4] From andhra_pradesh.json | Row 16 | Distance: 1.0437
2025-11-08 01:28:55,281 - __main__ - INFO -   [5] From UP.json | Row 19 | Distance: 1.0722
2025-11-08 01:28:55,282 - __main__ - INFO - Step 5: Generating final LLM prompt...
2025-11-08 01:28:55,282 - __main__ - INFO - Calling LLM for answer...
2025-11-08 01:28:55,282 - __main__ - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-08 01:28:56,856 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-08 01:28:58,958 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-08 01:28:58,960 - __main__ - INFO - Received response from LLM
2025-11-08 01:28:58,962 - __main__ - INFO - 
============================================================
2025-11-08 01:28:58,962 - __main__ - INFO - FINAL RESULTS
2025-11-08 01:28:58,962 - __main__ - INFO - ============================================================
2025-11-08 01:28:58,966 - __main__ - INFO - Pipeline completed successfully
2025-11-08 01:29:27,702 - __main__ - INFO - ============================================================
2025-11-08 01:29:27,702 - __main__ - INFO - Starting LLM RAG Pipeline
2025-11-08 01:29:27,703 - __main__ - INFO - ============================================================
2025-11-08 01:29:27,703 - __main__ - INFO - Step 1: Loading tables from 6 files...
2025-11-08 01:29:27,708 - __main__ - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 01:29:27,716 - __main__ - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 01:29:27,718 - __main__ - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 01:29:27,720 - __main__ - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 01:29:27,725 - __main__ - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 01:29:27,730 - __main__ - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 01:29:27,730 - __main__ - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 01:29:27,762 - __main__ - INFO - Created 150 total chunks from 6 files.
2025-11-08 01:29:27,763 - __main__ - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 01:29:27,772 - __main__ - INFO - All files unchanged, can use cache
2025-11-08 01:29:27,773 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-08 01:29:27,774 - __main__ - INFO - Loading embeddings and index from cache...
2025-11-08 01:29:27,775 - __main__ - INFO - FAISS index loaded: 150 vectors
2025-11-08 01:29:27,777 - __main__ - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 01:29:27,780 - __main__ - INFO - Chunks loaded: 150 chunks
2025-11-08 01:29:27,781 - __main__ - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 01:29:27,781 - __main__ - INFO - All cache files loaded successfully
2025-11-08 01:29:27,781 - __main__ - INFO - Using cached embeddings and index
2025-11-08 01:29:27,785 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 01:29:27,785 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 01:32:47,286 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 01:32:47,287 - llm - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 01:32:47,289 - llm - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 01:32:47,291 - llm - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 01:32:47,292 - llm - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 01:32:47,293 - llm - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 01:32:47,294 - llm - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 01:32:47,294 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 01:32:47,307 - llm - INFO - Created 150 total chunks from 6 files.
2025-11-08 01:32:47,307 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 01:32:47,319 - llm - INFO - All files unchanged, can use cache
2025-11-08 01:32:47,320 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 01:32:47,320 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 01:32:47,320 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 01:32:47,322 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 01:32:47,331 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 01:32:47,332 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 01:32:47,334 - llm - INFO - All cache files loaded successfully
2025-11-08 01:32:47,336 - llm - INFO - Using cached embeddings and index
2025-11-08 01:32:47,354 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 01:32:47,360 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 01:33:15,614 - llm - INFO - Step 4: Retrieving top 3 results for: 'hey cutu'
2025-11-08 01:33:15,866 - llm - INFO - Retrieved 3 chunks:
2025-11-08 01:33:15,866 - llm - INFO -   [1] From MP.json | Row 10 | Distance: 1.7589
2025-11-08 01:33:15,867 - llm - INFO -   [2] From bihar.json | Row 10 | Distance: 1.7662
2025-11-08 01:33:15,867 - llm - INFO -   [3] From bihar.json | Row 13 | Distance: 1.7820
2025-11-08 01:33:15,867 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 01:33:15,867 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.0-flash)...
2025-11-08 01:33:16,979 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2025-11-08 01:33:18,481 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
2025-11-08 01:33:18,497 - llm - INFO - Received response from LLM
2025-11-08 01:56:13,880 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 01:56:13,885 - llm - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 01:56:13,888 - llm - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 01:56:13,893 - llm - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 01:56:13,895 - llm - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 01:56:13,899 - llm - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 01:56:13,902 - llm - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 01:56:13,903 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 01:56:13,926 - llm - INFO - Created 150 total chunks from 6 files.
2025-11-08 01:56:13,927 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 01:56:13,933 - llm - INFO - All files unchanged, can use cache
2025-11-08 01:56:13,933 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 01:56:13,933 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 01:56:13,936 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 01:56:13,939 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 01:56:13,949 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 01:56:13,950 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 01:56:13,950 - llm - INFO - All cache files loaded successfully
2025-11-08 01:56:13,950 - llm - INFO - Using cached embeddings and index
2025-11-08 01:56:13,956 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 01:56:13,956 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 01:56:24,268 - llm - INFO - Step 4: Retrieving top 3 results for: 'hey'
2025-11-08 01:56:24,340 - llm - INFO - Retrieved 3 chunks:
2025-11-08 01:56:24,341 - llm - INFO -   [1] From punjab.json | Row 13 | Distance: 1.8518
2025-11-08 01:56:24,341 - llm - INFO -   [2] From bihar.json | Row 13 | Distance: 1.8641
2025-11-08 01:56:24,341 - llm - INFO -   [3] From punjab.json | Row 10 | Distance: 1.8648
2025-11-08 01:56:24,341 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 01:56:24,342 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-pro)...
2025-11-08 01:56:24,342 - llm - ERROR - Error getting LLM answer: module 'google.genai' has no attribute 'configure'
2025-11-08 02:09:36,731 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 02:09:36,737 - llm - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 02:09:36,740 - llm - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 02:09:36,743 - llm - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 02:09:36,746 - llm - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 02:09:36,748 - llm - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 02:09:36,750 - llm - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 02:09:36,751 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 02:09:36,811 - llm - INFO - Created 150 total chunks from 6 files.
2025-11-08 02:09:36,811 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 02:09:36,827 - llm - INFO - All files unchanged, can use cache
2025-11-08 02:09:36,828 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 02:09:36,828 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 02:09:36,831 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 02:09:36,833 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 02:09:36,837 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 02:09:36,840 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 02:09:36,840 - llm - INFO - All cache files loaded successfully
2025-11-08 02:09:36,842 - llm - INFO - Using cached embeddings and index
2025-11-08 02:09:36,851 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 02:09:36,852 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 02:10:00,542 - llm - INFO - Step 4: Retrieving top 3 results for: 'what up with punjab'
2025-11-08 02:10:01,307 - llm - INFO - Retrieved 3 chunks:
2025-11-08 02:10:01,310 - llm - INFO -   [1] From punjab.json | Row 19 | Distance: 0.9142
2025-11-08 02:10:01,315 - llm - INFO -   [2] From punjab.json | Row 14 | Distance: 0.9454
2025-11-08 02:10:01,317 - llm - INFO -   [3] From punjab.json | Row 23 | Distance: 1.0399
2025-11-08 02:10:01,320 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 02:10:01,325 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-pro)...
2025-11-08 02:10:02,518 - llm - ERROR - Error getting LLM answer: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-11-08 02:10:53,750 - llm - INFO - Step 4: Retrieving top 3 results for: 'hey'
2025-11-08 02:10:53,809 - llm - INFO - Retrieved 3 chunks:
2025-11-08 02:10:53,809 - llm - INFO -   [1] From punjab.json | Row 13 | Distance: 1.8518
2025-11-08 02:10:53,810 - llm - INFO -   [2] From bihar.json | Row 13 | Distance: 1.8641
2025-11-08 02:10:53,810 - llm - INFO -   [3] From punjab.json | Row 10 | Distance: 1.8648
2025-11-08 02:10:53,810 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 02:10:53,811 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-pro)...
2025-11-08 02:10:54,428 - llm - ERROR - Error getting LLM answer: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-11-08 02:10:57,662 - llm - INFO - Step 4: Retrieving top 3 results for: 'yo?'
2025-11-08 02:10:57,700 - llm - INFO - Retrieved 3 chunks:
2025-11-08 02:10:57,700 - llm - INFO -   [1] From punjab.json | Row 13 | Distance: 1.7617
2025-11-08 02:10:57,701 - llm - INFO -   [2] From bihar.json | Row 13 | Distance: 1.7780
2025-11-08 02:10:57,701 - llm - INFO -   [3] From UP.json | Row 13 | Distance: 1.7890
2025-11-08 02:10:57,703 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 02:10:57,704 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-pro)...
2025-11-08 02:10:58,336 - llm - ERROR - Error getting LLM answer: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-11-08 02:12:04,877 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 02:12:04,885 - llm - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 02:12:04,887 - llm - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 02:12:04,889 - llm - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 02:12:04,890 - llm - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 02:12:04,892 - llm - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 02:12:04,894 - llm - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 02:12:04,894 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 02:12:04,909 - llm - INFO - Created 150 total chunks from 6 files.
2025-11-08 02:12:04,909 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 02:12:04,918 - llm - INFO - All files unchanged, can use cache
2025-11-08 02:12:04,919 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 02:12:04,919 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 02:12:04,920 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 02:12:04,921 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 02:12:04,923 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 02:12:04,924 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 02:12:04,924 - llm - INFO - All cache files loaded successfully
2025-11-08 02:12:04,925 - llm - INFO - Using cached embeddings and index
2025-11-08 02:12:04,927 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 02:12:04,927 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 02:13:15,211 - llm - INFO - Step 4: Retrieving top 3 results for: 'hey'
2025-11-08 02:13:15,259 - llm - INFO - Retrieved 3 chunks:
2025-11-08 02:13:15,259 - llm - INFO -   [1] From punjab.json | Row 13 | Distance: 1.8518
2025-11-08 02:13:15,259 - llm - INFO -   [2] From bihar.json | Row 13 | Distance: 1.8641
2025-11-08 02:13:15,260 - llm - INFO -   [3] From punjab.json | Row 10 | Distance: 1.8648
2025-11-08 02:13:15,260 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 02:13:15,260 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-pro)...
2025-11-08 02:13:15,822 - llm - ERROR - Error getting LLM answer: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-11-08 02:15:09,204 - llm - INFO - Step 4: Retrieving top 3 results for: 'hey'
2025-11-08 02:15:09,260 - llm - INFO - Retrieved 3 chunks:
2025-11-08 02:15:09,262 - llm - INFO -   [1] From punjab.json | Row 13 | Distance: 1.8518
2025-11-08 02:15:09,262 - llm - INFO -   [2] From bihar.json | Row 13 | Distance: 1.8641
2025-11-08 02:15:09,262 - llm - INFO -   [3] From punjab.json | Row 10 | Distance: 1.8648
2025-11-08 02:15:09,263 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 02:15:09,263 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-pro)...
2025-11-08 02:15:09,617 - llm - ERROR - Error getting LLM answer: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-11-08 02:23:20,295 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 02:23:20,302 - llm - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 02:23:20,304 - llm - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 02:23:20,306 - llm - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 02:23:20,308 - llm - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 02:23:20,309 - llm - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 02:23:20,310 - llm - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 02:23:20,310 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 02:23:20,325 - llm - INFO - Created 150 total chunks from 6 files.
2025-11-08 02:23:20,326 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 02:23:20,337 - llm - INFO - All files unchanged, can use cache
2025-11-08 02:23:20,337 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 02:23:20,338 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 02:23:20,339 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 02:23:20,340 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 02:23:20,342 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 02:23:20,343 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 02:23:20,343 - llm - INFO - All cache files loaded successfully
2025-11-08 02:23:20,343 - llm - INFO - Using cached embeddings and index
2025-11-08 02:23:20,345 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 02:23:20,346 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 02:24:04,652 - llm - INFO - Step 4: Retrieving top 3 results for: 'whats up with punjab'
2025-11-08 02:24:04,747 - llm - INFO - Retrieved 3 chunks:
2025-11-08 02:24:04,748 - llm - INFO -   [1] From punjab.json | Row 19 | Distance: 0.9698
2025-11-08 02:24:04,748 - llm - INFO -   [2] From punjab.json | Row 14 | Distance: 0.9878
2025-11-08 02:24:04,748 - llm - INFO -   [3] From punjab.json | Row 15 | Distance: 1.0962
2025-11-08 02:24:04,749 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 02:24:04,749 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.5-flash)...
2025-11-08 02:24:12,279 - llm - INFO - Received response from LLM
2025-11-08 02:35:57,858 - llm - INFO - Step 4: Retrieving top 3 results for: 'whats up with punjab'
2025-11-08 02:35:58,002 - llm - INFO - Retrieved 3 chunks:
2025-11-08 02:35:58,002 - llm - INFO -   [1] From punjab.json | Row 19 | Distance: 0.9698
2025-11-08 02:35:58,002 - llm - INFO -   [2] From punjab.json | Row 14 | Distance: 0.9878
2025-11-08 02:35:58,002 - llm - INFO -   [3] From punjab.json | Row 15 | Distance: 1.0962
2025-11-08 02:35:58,002 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 02:35:58,002 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.5-flash)...
2025-11-08 02:36:10,730 - llm - INFO - Received response from LLM
2025-11-08 02:41:41,169 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 02:41:41,173 - llm - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 02:41:41,175 - llm - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 02:41:41,176 - llm - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 02:41:41,177 - llm - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 02:41:41,178 - llm - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 02:41:41,180 - llm - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 02:41:41,181 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 02:41:41,205 - llm - INFO - Created 150 total chunks from 6 files.
2025-11-08 02:41:41,206 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 02:41:41,214 - llm - INFO - All files unchanged, can use cache
2025-11-08 02:41:41,214 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 02:41:41,214 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 02:41:41,216 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 02:41:41,217 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 02:41:41,221 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 02:41:41,225 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 02:41:41,225 - llm - INFO - All cache files loaded successfully
2025-11-08 02:41:41,225 - llm - INFO - Using cached embeddings and index
2025-11-08 02:41:41,225 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 02:41:41,225 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 02:42:01,232 - llm - INFO - Step 4: Retrieving top 3 results for: 'what up with punjab'
2025-11-08 02:42:01,350 - llm - INFO - Retrieved 3 chunks:
2025-11-08 02:42:01,350 - llm - INFO -   [1] From punjab.json | Row 19 | Distance: 0.9142
2025-11-08 02:42:01,351 - llm - INFO -   [2] From punjab.json | Row 14 | Distance: 0.9454
2025-11-08 02:42:01,351 - llm - INFO -   [3] From punjab.json | Row 23 | Distance: 1.0399
2025-11-08 02:42:01,351 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 02:42:01,351 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.5-flash)...
2025-11-08 02:42:15,668 - llm - INFO - Received response from LLM
2025-11-08 03:21:10,082 - llm - INFO - Step 1: Loading tables from 6 files...
2025-11-08 03:21:10,091 - llm - INFO - Loaded 'punjab.json' (24 rows)
2025-11-08 03:21:10,096 - llm - INFO - Loaded 'all_india.json' (30 rows)
2025-11-08 03:21:10,098 - llm - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-11-08 03:21:10,140 - llm - INFO - Loaded 'bihar.json' (24 rows)
2025-11-08 03:21:10,146 - llm - INFO - Loaded 'UP.json' (24 rows)
2025-11-08 03:21:10,150 - llm - INFO - Loaded 'MP.json' (24 rows)
2025-11-08 03:21:10,150 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-11-08 03:21:10,197 - llm - INFO - Created 150 total chunks from 6 files.
2025-11-08 03:21:10,198 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-11-08 03:21:10,219 - llm - INFO - All files unchanged, can use cache
2025-11-08 03:21:10,219 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 03:21:10,219 - llm - INFO - Loading embeddings and index from cache...
2025-11-08 03:21:10,228 - llm - INFO - FAISS index loaded: 150 vectors
2025-11-08 03:21:10,230 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-11-08 03:21:10,235 - llm - INFO - Chunks loaded: 150 chunks
2025-11-08 03:21:10,236 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-11-08 03:21:10,237 - llm - INFO - All cache files loaded successfully
2025-11-08 03:21:10,237 - llm - INFO - Using cached embeddings and index
2025-11-08 03:21:10,245 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-11-08 03:21:10,245 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-08 03:21:31,994 - llm - INFO - Step 4: Retrieving top 3 results for: 'punjab'
2025-11-08 03:21:32,511 - llm - INFO - Retrieved 3 chunks:
2025-11-08 03:21:32,513 - llm - INFO -   [1] From punjab.json | Row 19 | Distance: 0.9588
2025-11-08 03:21:32,515 - llm - INFO -   [2] From punjab.json | Row 14 | Distance: 0.9989
2025-11-08 03:21:32,515 - llm - INFO -   [3] From punjab.json | Row 23 | Distance: 1.0267
2025-11-08 03:21:32,517 - llm - INFO - Step 5: Generating final LLM prompt...
2025-11-08 03:21:32,519 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.5-flash)...
2025-11-08 03:21:38,703 - llm - INFO - Received response from LLM
2025-12-01 07:23:10,523 - llm - INFO - Step 1: Loading tables from 6 files...
2025-12-01 07:23:10,534 - llm - INFO - Loaded 'punjab.json' (24 rows)
2025-12-01 07:23:10,537 - llm - INFO - Loaded 'all_india.json' (30 rows)
2025-12-01 07:23:10,543 - llm - INFO - Loaded 'andhra_pradesh.json' (24 rows)
2025-12-01 07:23:10,547 - llm - INFO - Loaded 'bihar.json' (24 rows)
2025-12-01 07:23:10,562 - llm - INFO - Loaded 'UP.json' (24 rows)
2025-12-01 07:23:10,562 - llm - INFO - Loaded 'MP.json' (24 rows)
2025-12-01 07:23:10,562 - llm - INFO - Step 2: Creating row-based chunks with metadata...
2025-12-01 07:23:10,751 - llm - INFO - Created 150 total chunks from 6 files.
2025-12-01 07:23:10,758 - llm - INFO - Step 3: Embedding chunks and building FAISS index...
2025-12-01 07:23:10,818 - llm - INFO - All files unchanged, can use cache
2025-12-01 07:23:10,826 - llm - INFO - Loading embeddings and index from cache...
2025-12-01 07:23:10,828 - llm - INFO - Loading embeddings and index from cache...
2025-12-01 07:23:10,843 - llm - INFO - FAISS index loaded: 150 vectors
2025-12-01 07:23:10,856 - llm - INFO - Embeddings loaded: shape (150, 384)
2025-12-01 07:23:10,869 - llm - INFO - Chunks loaded: 150 chunks
2025-12-01 07:23:10,875 - llm - INFO - Model name: all-MiniLM-L6-v2
2025-12-01 07:23:10,877 - llm - INFO - All cache files loaded successfully
2025-12-01 07:23:10,883 - llm - INFO - Using cached embeddings and index
2025-12-01 07:23:10,891 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-12-01 07:23:10,891 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-12-01 07:23:30,250 - llm - INFO - Step 4: Retrieving top 3 results for: 'hi'
2025-12-01 07:23:30,654 - llm - INFO - Retrieved 3 chunks:
2025-12-01 07:23:30,654 - llm - INFO -   [1] From MP.json | Row 14 | Distance: 1.7430
2025-12-01 07:23:30,657 - llm - INFO -   [2] From UP.json | Row 19 | Distance: 1.7566
2025-12-01 07:23:30,657 - llm - INFO -   [3] From MP.json | Row 12 | Distance: 1.7580
2025-12-01 07:23:30,657 - llm - INFO - Step 5: Generating final LLM prompt...
2025-12-01 07:23:30,658 - llm - INFO - Step 6: Getting answer from LLM (model: gemini-2.5-flash)...
2025-12-01 07:23:33,111 - llm - INFO - Received response from LLM
2025-12-01 07:55:56,697 - __main__ - ERROR - Error in query decomposition: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
Please retry in 2.934635081s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 2
}
]
2025-12-01 07:57:25,708 - __main__ - ERROR - Error in query decomposition: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
Please retry in 33.963425288s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 33
}
]
2025-12-01 07:57:26,447 - __main__ - ERROR - Error answering sub-query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
Please retry in 33.296338898s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 33
}
]
2025-12-01 07:59:16,603 - __main__ - ERROR - Error in query decomposition: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
Please retry in 43.137134705s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 43
}
]
2025-12-01 07:59:17,154 - __main__ - ERROR - Error answering sub-query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
Please retry in 42.506098062s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 42
}
]
2025-12-01 07:59:17,502 - __main__ - ERROR - Error combining answers: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
Please retry in 42.214977921s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 42
}
]
2025-12-01 07:59:17,801 - __main__ - ERROR - Error in query decomposition: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
Please retry in 41.967223642s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 41
}
]
2025-12-01 07:59:18,469 - __main__ - ERROR - Error answering sub-query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
Please retry in 41.253268706s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 41
}
]
2025-12-01 07:59:18,809 - __main__ - ERROR - Error combining answers: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
Please retry in 40.926179435s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 40
}
]
2025-12-01 07:59:19,508 - __main__ - ERROR - Error processing simple query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp
Please retry in 40.176284068s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-flash-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 40
}
]
2025-12-01 08:00:18,112 - __main__ - ERROR - Error in query decomposition: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:18,834 - __main__ - ERROR - Error answering sub-query: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:19,212 - __main__ - ERROR - Error combining answers: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:19,394 - __main__ - ERROR - Error in query decomposition: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:20,419 - __main__ - ERROR - Error answering sub-query: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:20,927 - __main__ - ERROR - Error combining answers: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:21,743 - __main__ - ERROR - Error processing simple query: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:52,315 - __main__ - ERROR - Error in query decomposition: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:52,782 - __main__ - ERROR - Error answering sub-query: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:53,050 - __main__ - ERROR - Error combining answers: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:53,275 - __main__ - ERROR - Error in query decomposition: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:53,766 - __main__ - ERROR - Error answering sub-query: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:54,248 - __main__ - ERROR - Error combining answers: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:00:54,824 - __main__ - ERROR - Error processing simple query: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 08:01:47,207 - __main__ - ERROR - Error answering sub-query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2, model: gemini-2.5-pro
Please retry in 12.583460803s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 2
}
, retry_delay {
  seconds: 12
}
]
2025-12-01 08:01:47,764 - __main__ - ERROR - Error answering sub-query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2, model: gemini-2.5-pro
Please retry in 11.901621754s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 2
}
, retry_delay {
  seconds: 11
}
]
2025-12-01 08:01:48,137 - __main__ - ERROR - Error combining answers: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2, model: gemini-2.5-pro
Please retry in 11.557390018s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 2
}
, retry_delay {
  seconds: 11
}
]
2025-12-01 08:01:48,377 - __main__ - ERROR - Error in query decomposition: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2, model: gemini-2.5-pro
Please retry in 11.341861031s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 2
}
, retry_delay {
  seconds: 11
}
]
2025-12-01 08:01:49,410 - __main__ - ERROR - Error answering sub-query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2, model: gemini-2.5-pro
Please retry in 10.348512684s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 2
}
, retry_delay {
  seconds: 10
}
]
2025-12-01 08:01:49,591 - __main__ - ERROR - Error combining answers: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2, model: gemini-2.5-pro
Please retry in 10.048732073s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 2
}
, retry_delay {
  seconds: 10
}
]
2025-12-01 08:01:50,282 - __main__ - ERROR - Error processing simple query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2, model: gemini-2.5-pro
Please retry in 9.363802374s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemini-2.5-pro"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 2
}
, retry_delay {
  seconds: 9
}
]
2025-12-01 08:38:58,887 - llm - ERROR - Error embedding batch 1: 
  No API_KEY or ADC found. Please either:
    - Set the `GOOGLE_API_KEY` environment variable.
    - Manually pass the key with `genai.configure(api_key=my_api_key)`.
    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.
2025-12-01 08:42:21,467 - llm - ERROR - Error embedding batch 1: 
  No API_KEY or ADC found. Please either:
    - Set the `GOOGLE_API_KEY` environment variable.
    - Manually pass the key with `genai.configure(api_key=my_api_key)`.
    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.
2025-12-01 08:51:12,296 - llm - ERROR - Error embedding batch 1: 400 API key not valid. Please pass a valid API key. [reason: "API_KEY_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "generativelanguage.googleapis.com"
}
, locale: "en-US"
message: "API key not valid. Please pass a valid API key."
]
2025-12-01 09:00:36,960 - llm - ERROR - Error embedding batch 1: 400 * BatchEmbedContentsRequest.model: unexpected model name format
* BatchEmbedContentsRequest.requests[0].model: unexpected model name format
* BatchEmbedContentsRequest.requests[1].model: unexpected model name format
* BatchEmbedContentsRequest.requests[2].model: unexpected model name format
* BatchEmbedContentsRequest.requests[3].model: unexpected model name format
* BatchEmbedContentsRequest.requests[4].model: unexpected model name format
* BatchEmbedContentsRequest.requests[5].model: unexpected model name format
* BatchEmbedContentsRequest.requests[6].model: unexpected model name format
* BatchEmbedContentsRequest.requests[7].model: unexpected model name format
* BatchEmbedContentsRequest.requests[8].model: unexpected model name format
* BatchEmbedContentsRequest.requests[9].model: unexpected model name format
* BatchEmbedContentsRequest.requests[10].model: unexpected model name format
* BatchEmbedContentsRequest.requests[11].model: unexpected model name format
* BatchEmbedContentsRequest.requests[12].model: unexpected model name format
* BatchEmbedContentsRequest.requests[13].model: unexpected model name format
* BatchEmbedContentsRequest.requests[14].model: unexpected model name format
* BatchEmbedContentsRequest.requests[15].model: unexpected model name format
* BatchEmbedContentsRequest.requests[16].model: unexpected model name format
* BatchEmbedContentsRequest.requests[17].model: unexpected model name format
* BatchEmbedContentsRequest.requests[18].model: unexpected model name format
* BatchEmbedContentsRequest.requests[19].model: unexpected model name format
* BatchEmbedContentsRequest.requests[20].model: unexpected model name format
* BatchEmbedContentsRequest.requests[21].model: unexpected model name format
* BatchEmbedContentsRequest.requests[22].model: unexpected model name format
* BatchEmbedContentsRequest.requests[23].model: unexpected model name format
* BatchEmbedContentsRequest.requests[24].model: unexpected model name format
* BatchEmbedContentsRequest.requests[25].model: unexpected model name format
* BatchEmbedContentsRequest.requests[26].model: unexpected model name format
* BatchEmbedContentsRequest.requests[27].model: unexpected model name format
* BatchEmbedContentsRequest.requests[28].model: unexpected model name format
* BatchEmbedContentsRequest.requests[29].model: unexpected model name format
* BatchEmbedContentsRequest.requests[30].model: unexpected model name format
* BatchEmbedContentsRequest.requests[31].model: unexpected model name format
* BatchEmbedContentsRequest.requests[32].model: unexpected model name format
* BatchEmbedContentsRequest.requests[33].model: unexpected model name format
* BatchEmbedContentsRequest.requests[34].model: unexpected model name format
* BatchEmbedContentsRequest.requests[35].model: unexpected model name format
* BatchEmbedContentsRequest.requests[36].model: unexpected model name format
* BatchEmbedContentsRequest.requests[37].model: unexpected model name format
* BatchEmbedContentsRequest.requests[38].model: unexpected model name format
* BatchEmbedContentsRequest.requests[39].model: unexpected model name format
* BatchEmbedContentsRequest.requests[40].model: unexpected model name format
* BatchEmbedContentsRequest.requests[41].model: unexpected model name format
* BatchEmbedContentsRequest.requests[42].model: unexpected model name format
* BatchEmbedContentsRequest.requests[43].model: unexpected model name format
* BatchEmbedContentsRequest.requests[44].model: unexpected model name format
* BatchEmbedContentsRequest.requests[45].model: unexpected model name format
* BatchEmbedContentsRequest.requests[46].model: unexpected model name format
* BatchEmbedContentsRequest.requests[47].model: unexpected model name format
* BatchEmbedContentsRequest.requests[48].model: unexpected model name format
* BatchEmbedContentsRequest.requests[49].model: unexpected model name format
* BatchEmbedContentsRequest.requests[50].model: unexpected model name format
* BatchEmbedContentsRequest.requests[51].model: unexpected model name format
* BatchEmbedContentsRequest.requests[52].model: unexpected model name format
* BatchEmbedContentsRequest.requests[53].model: unexpected model name format
* BatchEmbedContentsRequest.requests[54].model: unexpected model name format
* BatchEmbedContentsRequest.requests[55].model: unexpected model name format
* BatchEmbedContentsRequest.requests[56].model: unexpected model name format
* BatchEmbedContentsRequest.requests[57].model: unexpected model name format
* BatchEmbedContentsRequest.requests[58].model: unexpected model name format
* BatchEmbedContentsRequest.requests[59].model: unexpected model name format
* BatchEmbedContentsRequest.requests[60].model: unexpected model name format
* BatchEmbedContentsRequest.requests[61].model: unexpected model name format
* BatchEmbedContentsRequest.requests[62].model: unexpected model name format
* BatchEmbedContentsRequest.requests[63].model: unexpected model name format
* BatchEmbedContentsRequest.requests[64].model: unexpected model name format
* BatchEmbedContentsRequest.requests[65].model: unexpected model name format
* BatchEmbedContentsRequest.requests[66].model: unexpected model name format
* BatchEmbedContentsRequest.requests[67].model: unexpected model name format
* BatchEmbedContentsRequest.requests[68].model: unexpected model name format
* BatchEmbedContentsRequest.requests[69].model: unexpected model name format
* BatchEmbedContentsRequest.requests[70].model: unexpected model name format
* BatchEmbedContentsRequest.requests[71].model: unexpected model name format
* BatchEmbedContentsRequest.requests[72].model: unexpected model name format
* BatchEmbedContentsRequest.requests[73].model: unexpected model name format
* BatchEmbedContentsRequest.requests[74].model: unexpected model name format
* BatchEmbedContentsRequest.requests[75].model: unexpected model name format
* BatchEmbedContentsRequest.requests[76].model: unexpected model name format
* BatchEmbedContentsRequest.requests[77].model: unexpected model name format
* BatchEmbedContentsRequest.requests[78].model: unexpected model name format
* BatchEmbedContentsRequest.requests[79].model: unexpected model name format
* BatchEmbedContentsRequest.requests[80].model: unexpected model name format
* BatchEmbedContentsRequest.requests[81].model: unexpected model name format
* BatchEmbedContentsRequest.requests[82].model: unexpected model name format
* BatchEmbedContentsRequest.requests[83].model: unexpected model name format
* BatchEmbedContentsRequest.requests[84].model: unexpected model name format
* BatchEmbedContentsRequest.requests[85].model: unexpected model name format
* BatchEmbedContentsRequest.requests[86].model: unexpected model name format
* BatchEmbedContentsRequest.requests[87].model: unexpected model name format
* BatchEmbedContentsRequest.requests[88].model: unexpected model name format
* BatchEmbedContentsRequest.requests[89].model: unexpected model name format
* BatchEmbedContentsRequest.requests[90].model: unexpected model name format
* BatchEmbedContentsRequest.requests[91].model: unexpected model name format
* BatchEmbedContentsRequest.requests[92].model: unexpected model name format
* BatchEmbedContentsRequest.requests[93].model: unexpected model name format
* BatchEmbedContentsRequest.requests[94].model: unexpected model name format
* BatchEmbedContentsRequest.requests[95].model: unexpected model name format
* BatchEmbedContentsRequest.requests[96].model: unexpected model name format
* BatchEmbedContentsRequest.requests[97].model: unexpected model name format
* BatchEmbedContentsRequest.requests[98].model: unexpected model name format
* BatchEmbedContentsRequest.requests[99].model: unexpected model name format

2025-12-01 09:12:52,326 - llm - ERROR - Error generating LLM response: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-01 09:35:36,100 - llm - ERROR - Error generating LLM response: 404 models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-02 01:36:39,138 - llm - ERROR - File not found: UP.json
2025-12-02 01:42:23,383 - llm - ERROR - File not found: UP.json
2025-12-02 01:51:03,102 - __main__ - ERROR - Error loading 'Uttarakhand.json': Extra data: line 365 column 1 (char 31410)
2025-12-02 01:52:34,574 - llm - ERROR - File not found: UP.json
